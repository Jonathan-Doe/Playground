scale_colour_manual(values = c("blue", "lightblue", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),
size=c(2, 1, 1)),
title = "")) + ggtitle("Sliding window regression with outliers")+
xlab("x")+ylab("y")
epsilon[x] <- rnorm(n_outl, mean = 150, sd=2)
epsilon[x[x%%2==TRUE]] <- -epsilon[x[x%%2==TRUE]]
################# outliers
create_sample_with_outliers <- function(n, n_outl){
X <- runif(n, min = 0, max = 3)
x <- sample(1:n, n_outl)
epsilon <- runif(n, min = -1, max = 1)
epsilon[x] <- rnorm(n_outl, mean = 150, sd=2)
epsilon[x[x%%2==TRUE]] <- -epsilon[x[x%%2==TRUE]]
return(data.frame(x=X, y=(theoretical_regression(X)+epsilon)))
}
df_out <- create_sample_with_outliers(300, 5)
ggplot(df_out, aes(x=x, y=y, colour="Data"))+geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+ stat_function(fun=rolling_mean,
args=list(X=df_out$x, Y=df_out$y, h=0.04),
aes(colour = "Sliding window regression"), geom="line", size=1) +
scale_colour_manual(values = c("blue", "lightblue", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),
size=c(2, 1, 1)),
title = "")) + ggtitle("Sliding window regression with outliers")+
xlab("x")+ylab("y")
epsilon[x] <- rnorm(n_outl, mean = 1500, sd=2)
################# outliers
create_sample_with_outliers <- function(n, n_outl){
X <- runif(n, min = 0, max = 3)
x <- sample(1:n, n_outl)
epsilon <- runif(n, min = -1, max = 1)
epsilon[x] <- rnorm(n_outl, mean = 1500, sd=2)
epsilon[x[x%%2==TRUE]] <- -epsilon[x[x%%2==TRUE]]
return(data.frame(x=X, y=(theoretical_regression(X)+epsilon)))
}
df_out <- create_sample_with_outliers(300, 5)
ggplot(df_out, aes(x=x, y=y, colour="Data"))+geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+ stat_function(fun=rolling_mean,
args=list(X=df_out$x, Y=df_out$y, h=0.04),
aes(colour = "Sliding window regression"), geom="line", size=1) +
scale_colour_manual(values = c("blue", "lightblue", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),
size=c(2, 1, 1)),
title = "")) + ggtitle("Sliding window regression with outliers")+
xlab("x")+ylab("y")
### running med
rolling_median <- function(x, X, Y, h){
return(sapply(x, function(x){ median(Y[abs(X-x)<= h])}))}
ggplot(df, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=rolling_median,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "Sliding median regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "green", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
ggplot(df_out, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=rolling_median,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "Sliding median regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "green", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
epanechnikov_kernel <- function(x){
return(sapply(x, function(x){ 3*(1-x^2)/4*(abs(x)<=1)}))}
local_linear <- function(x, X, Y, h){
one_ <- sapply(x, function(x){mean(epanechnikov_kernel((x-X)/h)/h)})
Y_ <- sapply(x, function(x){mean(Y*epanechnikov_kernel((x-X)/h)/h)})
X_ <- sapply(x, function(x){mean((X-x)*epanechnikov_kernel((x-X)/h)/h)})
XY_ <- sapply(x, function(x){mean(Y*(X-x)*epanechnikov_kernel((x-X)/h)/h)
})
XX_ <- sapply(x, function(x){mean((X-x)*(X-x)*epanechnikov_kernel((x-X)/h
)/h)})
return((Y_*XX_-X_*XY_)/(XX_*one_-(X_)^2))
}
ggplot(df_out, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=local_linear,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "Sliding median regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "green", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
ggplot(df_out, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=local_linear,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "local linear regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "green", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
ggplot(df, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=local_linear,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "local linear regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "green", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
ggplot(df_out, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=local_linear,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "local linear regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "green", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
ggplot(df_out, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=local_linear,
args=list(X=df_out$x, Y=df_out$y, h=0.1),
aes(colour = "local linear regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "green", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
library(ggplot2)
library(rmutil)
library(stats)
library(ggplot2)
library(rmutil)
library(stats)
size_list <- c(10, 50, 100, 500, 1000)
samples <- lapply(size_list, function(x){rlaplace(x, m=0, s=1)})
gaus_kernel <- function(x, sample, h){
n <- length(sample)
s <- 0
for(i in 1:n)   s <- s + exp(-0.5*(x-sample[i])^2/h^2)
return((n*h)^(-1)*s/sqrt(2*pi))
}
silverman_rule <- function(sample, simple = TRUE){
n <- length(sample)
c <- (pi*8*sqrt(2)/3)^(1/5)
v <- var(sample)
if (simple==TRUE){
return(c*v*n^(-1/5))
}
if (simple==FALSE){
return(c*min(IQR(sample)/1.34, v)*n^(-1/5))
}
if ((simple!=TRUE) && (simple != FALSE)){
stop("No such Silverman Rule")
}
}
y_cor=seq(from=0.46, by=-0.02, length.out=5)
rbPal <- colorRampPalette(c('green','blue'))
curve(exp(-abs(x))/2, from = -5, to = 5,col = "red", xlab = "x", ylab = "y", main="Density with Kernel Estimation(simple)")
h <- mapply(
function(a, b, c, d){ h <- silverman_rule(a)
curve(gaus_kernel(x, a, h), add=TRUE, col=d)
legend(2, b, c(paste("n=", c, sep="")), col = d, lwd = 2, bty = "n", cex = 0.90)
return(h) }, a = samples, b=y_cor, c=size_list, d=rbPal(5))
#improved
curve(exp(-abs(x))/2, col = "red", from = -5, to = 5, xlab = "x", ylab = "y", main="kernel density estimation(improved)")
#legend(2, 0.5, c("Laplace density"), col = c("red"),title = "Improved Silverman Rule",lwd = 2, bty = "n", cex = 0.90)
h_improved <- mapply(
function(a, b, c, d){  h <- silverman_rule(a, simple = FALSE)
curve(gaus_kernel(x, a, h), add=TRUE, col=d)
legend(2, b, c(paste("n=", c, sep="")), col = d, lwd = 2, bty = "n", cex = 0.90)
return(h)}, a = samples, b=y_cor, c=size_list, d=rbPal(5))
curve(exp(-abs(x))/2, col = "red", from = -5, to = 5,xlab = "x", ylab = "y", main="kernel density estimation")
#legend(1.8, 0.5, c("Laplace density"), col = c("red"),title = "Nonparametric estimation",lwd = 2, bty = "n", cex = 0.90)
h_nonparam <- mapply(function(a, b, c, d, e){
h_est <- e
nn <- c
A <- matrix(a, ncol = nn, nrow=nn)
tA <- t(A)
AA <- ((tA-A)^4-10*(h_est*(tA-A))^2+11*h_est^4)*exp(-(tA-A)^2/(2*h_est^2))
S1 <- 3*sqrt(pi)*h_est*nn/4
S2 <- (sum(AA)-sum(diag(AA)))*sqrt(pi)/(16*h_est^3*sqrt(2))
phi <- (S1 + S2)/(2*pi*h_est^6*nn^2)
h_nonparametric <- (2*sqrt(pi)*phi*nn)^(-1/5)
curve(gaus_kernel(x, a, h_nonparametric), add=TRUE, col=d)
legend(2, b, c(paste("n=", c, sep="")), col = d, lwd = 2, bty = "n", cex = 0.90)
return(h_nonparametric) }, a = samples, b=y_cor, c=size_list, d=rbPal(5), e = h_improved)
curve(exp(-abs(x))/2, col = "red", from = -5, to = 5, xlab = "x", ylab = "y", main="Nonparametric method\nSimple Silverman Rule")
h_nonparam_s <- mapply(function(a, b, c, d, e){
h_est <- e
nn <- c
A <- matrix(a, ncol = nn, nrow=nn)
tA <- t(A)
AA <- ((tA-A)^4-10*(h_est*(tA-A))^2+11*h_est^4)*exp(-(tA-A)^2/(2*h_est^2))
S1 <- 3*sqrt(pi)*h_est*nn/4
S2 <- (sum(AA)-sum(diag(AA)))*sqrt(pi)/(16*h_est^3*sqrt(2))
phi <- (S1 + S2)/(2*pi*h_est^6*nn^2)
h_nonparametric <- (2*sqrt(pi)*phi*nn)^(-1/5)
curve(gaus_kernel(x, a, h_nonparametric), add=TRUE, col=d)
legend(2, b, c(paste("n=", c, sep="")), col = d, lwd = 2, bty = "n", cex = 0.90)
return(h_nonparametric)}, a = samples, b=y_cor, c=size_list, d=rbPal(5), e = h )
################### CV
cross_validation <- function(h, sample) {
nn <- length(sample)
A <- matrix(sample, ncol = nn, nrow=nn)
tA <- t(A)
summ1 <- exp(-(tA-A)^2/(4*h^2))
summ2 <- exp(-(tA-A)^2/(2*h^2))
CV <- sum(summ1)/(2*nn^2*h*sqrt(pi))-2*sum(summ2)/(sqrt(2*pi)*nn*(nn-1)*h)+ 2/(sqrt(2*pi)*(nn-1)*h)
return(CV)
}
# ---------------------------------------------
h_cv <- mapply(function(samples, hh){
h_cv <- nlm(cross_validation , hh, sample =samples)$estimate
return(h_cv) }, samples=samples, hh=h)
df <- mapply(function(hh, samples){
h_grid <- seq(hh/100, hh*10, 0.01)
h_1 <- lapply(h_grid, cross_validation, sample= samples)
df1 <- data.frame(h_grid=h_grid, h_1=unlist(h_1))
return(list(df1))
}, samples=samples, hh=h)
for(i in 1:5){
curve(exp(-abs(x))/2, from = -5, to = 5,col = "red", xlab = "x", ylab = "y", main=paste("kernel density estimation\nn=", size_list[i], sep=""))
#legend(2, 0.45, c("Laplace density"), col = c("red"),title = "Estimations",lwd = 2, bty = "n", cex = 0.90)
curve(gaus_kernel(x, samples[[i]], h[i]), add=TRUE, col="green")
curve(gaus_kernel(x, samples[[i]], h_improved[i]), add=TRUE, col="blue")
curve(gaus_kernel(x, samples[[i]], h_nonparam[i]), add=TRUE, col="orange")
curve(gaus_kernel(x, samples[[i]], h_nonparam_s[i]), add=TRUE, col="purple")
curve(gaus_kernel(x, samples[[i]], h_cv[i]), add=TRUE, col="pink")
legend(2, 0.4, "Simple Silverman Rule", col = "green",lwd = 2, bty = "n", cex = 0.90)
legend(2, 0.4-.04, "Improved Silverman Rule", col = "blue",lwd = 2, bty = "n", cex = 0.90)
legend(2, 0.4-2*.04, "Nonparametric with ISR", col = "orange",lwd = 2, bty = "n", cex = 0.90)
legend(2, 0.4-3*.04, "Nonparametric with SSR", col = "purple",lwd = 2, bty = "n", cex = 0.90)
legend(2, 0.4-4*.04, "Cross-validation", col = "pink",lwd = 2, bty = "n", cex = 0.90)
}
for(i in 1:5){
curve(exp(-abs(x))/2, col = "red", from = -5, to = 5,
xlab = "x", ylab = "y",
main=paste("Cross-validation technique\nn=",
size_list[i], sep=""))
curve(gaus_kernel(x, samples[[i]], h_cv[i]), add=TRUE)
}
for(i in 1:5){
curve(exp(-abs(x))/2, from = -5, to = 5,col = "red", xlab = "x", ylab = "y", main=paste("kernel density estimation\nn=", size_list[i], sep=""))
#legend(2, 0.45, c("Laplace density"), col = c("red"),title = "Estimations",lwd = 2, bty = "n", cex = 0.90)
curve(gaus_kernel(x, samples[[i]], h[i]), add=TRUE, col="green")
curve(gaus_kernel(x, samples[[i]], h_improved[i]), add=TRUE, col="blue")
curve(gaus_kernel(x, samples[[i]], h_nonparam[i]), add=TRUE, col="orange")
curve(gaus_kernel(x, samples[[i]], h_nonparam_s[i]), add=TRUE, col="purple")
curve(gaus_kernel(x, samples[[i]], h_cv[i]), add=TRUE, col="pink")
legend(2, 0.4, "Simple Silverman Rule", col = "green",lwd = 2, bty = "n", cex = 0.90)
legend(2, 0.4-.04, "Improved Silverman Rule", col = "blue",lwd = 2, bty = "n", cex = 0.90)
legend(2, 0.4-2*.04, "Nonparametric with ISR", col = "orange",lwd = 2, bty = "n", cex = 0.90)
legend(2, 0.4-3*.04, "Nonparametric with SSR", col = "purple",lwd = 2, bty = "n", cex = 0.90)
legend(2, 0.4-4*.04, "Cross-validation", col = "pink",lwd = 2, bty = "n", cex = 0.90)
}
library(ggplot2)
n <- 300
theoretical_regression <- function(x){
return(-x*(-242+805*x-742*x^2+200*x^3)/7)
}
create_sample <- function(n){
X <- runif(n, min = 0, max = 3)
epsilon <- runif(n, min = -1, max = 1)
return(data.frame(x=X, y=(theoretical_regression(X)+epsilon)))
}
df <- create_sample(n)
rolling_mean <- function(x, X, Y, h){
numerator <- sapply(x, function(x)sum(Y[abs(X-x)<h]))
denominator <- sapply(x, function(x)sum(abs(X-x)<h))
return(numerator/denominator)
}
ggplot(df, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"),
geom="line", size=1)+
stat_function(fun=rolling_mean, args=list(X=df$x, Y=df$y, h=0.04),
aes(colour = "Sliding window regression"), geom="line", size=1)+ scale_colour_manual(values = c("black", "green", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),shape=c(16, NA, NA),
size=c(2, 1, 1)),title = "")) + ggtitle("Sliding window regression") + xlab("x")+ylab("y")
ggplot(df, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"),
geom="line", size=1)+
stat_function(fun=rolling_mean, args=list(X=df$x, Y=df$y, h=0.04),
aes(colour = "Sliding window regression"), geom="line", size=1)+ scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),shape=c(16, NA, NA),
size=c(2, 1, 1)),title = "")) + ggtitle("Sliding window regression") + xlab("x")+ylab("y")
################# outliers
create_sample_with_outliers <- function(n, n_outl){
X <- runif(n, min = 0, max = 3)
x <- sample(1:n, n_outl)
epsilon <- runif(n, min = -1, max = 1)
epsilon[x] <- rnorm(n_outl, mean = 1500, sd=2)
epsilon[x[x%%2==TRUE]] <- -epsilon[x[x%%2==TRUE]]
return(data.frame(x=X, y=(theoretical_regression(X)+epsilon)))
}
df_out <- create_sample_with_outliers(300, 5)
ggplot(df_out, aes(x=x, y=y, colour="Data"))+geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+ stat_function(fun=rolling_mean,
args=list(X=df_out$x, Y=df_out$y, h=0.04),
aes(colour = "Sliding window regression"), geom="line", size=1) +
scale_colour_manual(values = c("blue", "lightblue", "red"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),
size=c(2, 1, 1)),
title = "")) + ggtitle("Sliding window regression with outliers")+
xlab("x")+ylab("y")
ggplot(df_out, aes(x=x, y=y, colour="Data"))+geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+ stat_function(fun=rolling_mean,
args=list(X=df_out$x, Y=df_out$y, h=0.04),
aes(colour = "Sliding window regression"), geom="line", size=1) +
scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),
size=c(2, 1, 1)),
title = "")) + ggtitle("Sliding window regression with outliers")+
xlab("x")+ylab("y")
ggplot(df, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=rolling_median,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "Sliding median regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
### running med
rolling_median <- function(x, X, Y, h){
return(sapply(x, function(x){ median(Y[abs(X-x)<= h])}))}
ggplot(df, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=rolling_median,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "Sliding median regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
ggplot(df_out, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=rolling_median,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "Sliding median regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
epanechnikov_kernel <- function(x){
return(sapply(x, function(x){ 3*(1-x^2)/4*(abs(x)<=1)}))}
local_linear <- function(x, X, Y, h){
one_ <- sapply(x, function(x){mean(epanechnikov_kernel((x-X)/h)/h)})
Y_ <- sapply(x, function(x){mean(Y*epanechnikov_kernel((x-X)/h)/h)})
X_ <- sapply(x, function(x){mean((X-x)*epanechnikov_kernel((x-X)/h)/h)})
XY_ <- sapply(x, function(x){mean(Y*(X-x)*epanechnikov_kernel((x-X)/h)/h)
})
XX_ <- sapply(x, function(x){mean((X-x)*(X-x)*epanechnikov_kernel((x-X)/h
)/h)})
return((Y_*XX_-X_*XY_)/(XX_*one_-(X_)^2))
}
ggplot(df_out, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=local_linear,
args=list(X=df_out$x, Y=df_out$y, h=0.1),
aes(colour = "local linear regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
library(ggplot2)
n <- 300
theoretical_regression <- function(x){
return(-x*(-242+805*x-742*x^2+200*x^3)/7)
}
create_sample <- function(n){
X <- runif(n, min = 0, max = 3)
epsilon <- runif(n, min = -1, max = 1)
return(data.frame(x=X, y=(theoretical_regression(X)+epsilon)))
}
n <- 300
theoretical_regression <- function(x){
return(-x*(-242+805*x-742*x^2+200*x^3)/7)
}
create_sample <- function(n){
X <- runif(n, min = 0, max = 3)
epsilon <- runif(n, min = -1, max = 1)
return(data.frame(x=X, y=(theoretical_regression(X)+epsilon)))
}
df <- create_sample(n)
rolling_mean <- function(x, X, Y, h){
numerator <- sapply(x, function(x)sum(Y[abs(X-x)<h]))
denominator <- sapply(x, function(x)sum(abs(X-x)<h))
return(numerator/denominator)
}
ggplot(df, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"),
geom="line", size=1)+
stat_function(fun=rolling_mean, args=list(X=df$x, Y=df$y, h=0.04),
aes(colour = "Sliding window regression"), geom="line", size=1)+ scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),shape=c(16, NA, NA),
size=c(2, 1, 1)),title = "")) + ggtitle("Sliding window regression") + xlab("x")+ylab("y")
################# outliers
create_sample_with_outliers <- function(n, n_outl){
X <- runif(n, min = 0, max = 3)
x <- sample(1:n, n_outl)
epsilon <- runif(n, min = -1, max = 1)
epsilon[x] <- rnorm(n_outl, mean = 1500, sd=2)
epsilon[x[x%%2==TRUE]] <- -epsilon[x[x%%2==TRUE]]
return(data.frame(x=X, y=(theoretical_regression(X)+epsilon)))
}
df_out <- create_sample_with_outliers(300, 5)
ggplot(df_out, aes(x=x, y=y, colour="Data"))+geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+ stat_function(fun=rolling_mean,
args=list(X=df_out$x, Y=df_out$y, h=0.04),
aes(colour = "Sliding window regression"), geom="line", size=1) +
scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),
size=c(2, 1, 1)),
title = "")) + ggtitle("Sliding window regression with outliers")+
xlab("x")+ylab("y")
### running med
rolling_median <- function(x, X, Y, h){
return(sapply(x, function(x){ median(Y[abs(X-x)<= h])}))}
ggplot(df, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=rolling_median,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "Sliding median regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
ggplot(df_out, aes(x=x, y=y, colour="Data"))+ geom_point()+ stat_function(fun=theoretical_regression,
aes(colour = "Theoretical function"), geom="line", size=1)+stat_function(fun=rolling_median,
args=list(X=df$x, Y=df$y, h=0.1),
aes(colour = "Sliding median regression"), geom="line", size=1)+
scale_colour_manual(values = c("black", "red", "blue"),
guide = guide_legend(override.aes = list(linetype = c("blank", "solid", "solid"),
shape=c(16, NA, NA),size=c(2, 1, 1)),title = ""))+
ggtitle("Sliding meadian regression")+ xlab("x")+ylab("y")
#utf-8
setwd('/home/johndoe/repos/Playground/Multivariate analysis/')
library("factoextra")
library("CCA")
library("cluster")
library("fossil")
library("MASS")
library("gdata")
library("dplyr")
library("tidyr")
# Read data
data <- read.table("Сlustering with known #0f clusters data/mult7.txt")
######################## K means #####################
#WSS
fviz_nbclust(data, kmeans, method = "wss",k.max = 20)
#Silhoutte
fviz_nbclust(data, kmeans, method = "silhouette",k.max = 20)
# Vizualization on 2 principal comps
km <- kmeans(data, 6, nstart = 20)
pal <-c ("black","red","blue","green","magenta","chocolate")
plot(princomp(data)$scores[,1:2],col=pal[km$cluster],cex=0.2)
# Vizualization on canonical comps
k <- length(levels(as.factor(km$cluster)))
n <- nrow(data)
C <- matrix(data=as.numeric(rep(km$cluster,k) == rep(1:k, each=n)),ncol=k, nrow=n)
cc_res <- rcc(data, C, 0.1, 0.1)
plot(cc_res$scores$xscores[,3:4],col=pal[km$cluster],cex=0.2, xlab = '3rd comp', ylab = '4th comp')
#WSS
fviz_nbclust(data, pam, method = "wss",k.max = 20)
#Silhoutte
fviz_nbclust(data, pam, method = "silhouette",k.max = 20)
# Vizualization on 2 principal comps
kp <- pam(data, 6,metric = 'euclidean')
pal <-c ("black","red","blue","green","magenta","chocolate")
plot(princomp(data)$scores[,1:2],col=pal[kp$cluster],cex=0.2)
# Vizualization on canonical comps
k <- length(levels(as.factor(kp$cluster)))
n <- nrow(data)
C <- matrix(data=as.numeric(rep(kp$cluster,k) == rep(1:k, each=n)),ncol=k, nrow=n)
cc_res <- rcc(data, C, 0.1, 0.1)
plot(cc_res$scores$xscores[,3:4],col=pal[kp$cluster],cex=0.2, xlab = '3rd comp', ylab = '4th comp')
#Calcualte Rends index
rand.index(kp$clustering, km$cluster)
table(kp$clustering, km$cluster)
############# External Dataset - HTRU2 (https://archive.ics.uci.edu/ml/datasets/HTRU2)
data <- data.frame(read.csv("HTRU2/HTRU_2.csv",header = FALSE)) %>% drop_na()
n <- 1000
sampled_data <- sample_n(data, n, replace = FALSE)
fviz_nbclust(sampled_data, kmeans, method = "wss",k.max = 10)
fviz_nbclust(sampled_data, kmeans, method = "wss",k.max = 20)
#Silhoutte0
fviz_nbclust(data, kmaens, method = "silhouette",k.max = 20)
#Silhoutte0
fviz_nbclust(sampled_data, pam, method = "silhouette",k.max = 20)
#utf-8
setwd('/home/johndoe/repos/Playground/Multivariate analysis/')
library("factoextra")
library("CCA")
library("cluster")
library("fossil")
library("MASS")
library("gdata")
library("dplyr")
library("tidyr")
# Read data
data <- read.table("Сlustering with known #0f clusters data/mult7.txt")
d <- dist(x = data, method = 'euclidean')
d
d <- dist(x = dat)
d <- dist(x = data)
d
d <- dist(x = data, method = "euclidean")
source('~/repos/Playground/Multivariate analysis/multidimensional scaling.R', echo=TRUE)
# Read data
data <- data.frame(read.table("Сlustering with known #0f clusters data/mult7.txt"))
d <- dist(x = data, method = "euclidean")
d
mds <- cmdscale(d, k=2)
mds
km <- kmeans(data, centers = 6)
pal <- rainbow(6)
mds <- cmdscale(d, k=2, eig=TRUE)
x <- mds$points[,1]
y <- mds$points[,2]
plot(x, y,col=pal[km.res$cluster],cex=0.2)
plot(x, y,col=pal[km$cluster],cex=0.2)
